# -*- coding: utf-8 -*-
"""BART.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lx3vtqWS80zDsMQOSBmD29upOZx2JNcP
"""

import pandas as pd
from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments, EarlyStoppingCallback
import torch

# Specify the path to your Excel file
file_path = r"/content/2 documents_summary.xlsx"

# Read the Excel file
data_xlsx = pd.read_excel(file_path)

# Display the first few rows to verify the data
print(data_xlsx.head())
print(len(data_xlsx))
from datasets import Dataset

# Convert the dataset to a list of dictionaries where each dictionary contains 4 documents and their corresponding summary
formatted_data = [{"document": doc, "summary": summary} for doc, summary in zip(data_xlsx['documents'], data_xlsx['summaries'])]

# Split the dataset into training and testing (80% training, 20% testing)
split_index = int(0.8 * len(formatted_data))
train_data = formatted_data[:split_index]
test_data = formatted_data[split_index:]

# Convert to Hugging Face Dataset format
train_dataset = Dataset.from_list(train_data)
test_dataset = Dataset.from_list(test_data)

# Print a sample to verify
print("\nSample from training dataset (before tokenization):", train_dataset[0])
print("\nSample from test dataset (before tokenization):", test_dataset[0])

# Load the tokenizer and model from pretrained BART
tokenizer = BartTokenizer.from_pretrained("facebook/bart-large")
model = BartForConditionalGeneration.from_pretrained("facebook/bart-large")

# Adding a padding token is optional as BART already has one
tokenizer.pad_token = tokenizer.eos_token  # BART uses the same token for pad and eos

# Resize the token embeddings of the model to accommodate the new pad_token (if changed)
model.resize_token_embeddings(len(tokenizer))

# Define a function to tokenize the dataset
def preprocess_function(examples):
    inputs = ["summarize: " + doc for doc in examples["document"]]
    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding="max_length", return_tensors="pt")

    # Tokenize the summaries
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples["summary"], max_length=700, truncation=True, padding="max_length", return_tensors="pt")

    # Replace the padding token id in labels with -100 to ignore these during loss calculation
    labels["input_ids"] = [
        [(label if label != tokenizer.pad_token_id else -100) for label in label_list] for label_list in labels["input_ids"]
    ]

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Assuming train_dataset and test_dataset are loaded and formatted correctly
train_dataset = train_dataset.map(preprocess_function, batched=True)
test_dataset = test_dataset.map(preprocess_function, batched=True)

# Set up training arguments
training_args = TrainingArguments(
    output_dir="./bart_TRAINING",
    eval_strategy="steps",             # Evaluate every few steps
    per_device_train_batch_size=4,           # Batch size per GPU/TPU core
    per_device_eval_batch_size=4,            # Batch size for evaluation
    num_train_epochs=100,                     # Number of training epochs
    save_steps=500,                          # Save checkpoint every 100 steps
    eval_steps=500,                          # Evaluate every 100 steps
    logging_dir='./logs',                    # Directory to save logs
    logging_steps=500,                       # Log every 100 steps
    logging_strategy="steps",                # Log every few steps
    save_total_limit=3,                      # Limit number of checkpoints to 3
    load_best_model_at_end=True,             # Load the best model based on eval metrics
    metric_for_best_model="eval_loss",       # Metric for determining the best model
    report_to="none"                         # Disable reporting to external services (like W&B)
    )

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)

# Train and evaluate the model
trainer.train()
evaluation_results = trainer.evaluate()

# Save the fine-tuned model
model.save_pretrained("./bart-finetuned-legal")
tokenizer.save_pretrained("./bart-finetuned-legal")

# Print evaluation results
print(evaluation_results)