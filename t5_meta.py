# -*- coding: utf-8 -*-
"""T5_META.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12d1HlSkCoLxbztIi7piXujBAaDjBZ920
"""

import torch
import pandas as pd
import numpy as np
from datasets import Dataset
from evaluate import load
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    DataCollatorForSeq2Seq,
    EarlyStoppingCallback
)
from accelerate import Accelerator

# Clear GPU memory and prepare Accelerator
torch.cuda.empty_cache()
accelerator = Accelerator()

# === 1. Load and prepare the data ===
def combine_inputs(row):
    """Combine summaries from different models with clear labels for each source"""
    return (
        "özetle: "  # Use Turkish prompt
        "LED Özeti: " + str(row["LED_Generated_Summary"]) + " "
        "LongT5 Özeti: " + str(row["LONG_T5_Generated_Summary"]) + " "
        "BART Özeti: " + str(row["BART_Generated_Summary"]) + " "
        "GPT Özeti: " + str(row["GPT-3.5_Generated_Summary"])
    )

# Load the dataset and fill missing values
df = pd.read_excel("/content/multi_model_summaries.xlsx")
df = df.fillna("")
df["input_text"] = df.apply(combine_inputs, axis=1)
df["target_text"] = df["Reference_Summary"].astype(str)

# Split the dataset
dataset = Dataset.from_pandas(df[["input_text", "target_text"]])
dataset = dataset.train_test_split(test_size=0.2, seed=42)  # Add seed for reproducibility

# === 2. Load model and tokenizer ===
model_name = "google/long-t5-tglobal-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# === 3. Preprocess the data ===
max_input_len = MAX  # Suitable for long documents
max_target_len = MIN   # Reasonable summary length

def preprocess(examples):
    """Tokenize inputs and targets"""
    inputs = [text for text in examples["input_text"]]
    targets = [text for text in examples["target_text"]]

    model_inputs = tokenizer(
        inputs,
        max_length=max_input_len,
        padding="max_length",
        truncation=True,
        return_tensors="pt"
    )

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            targets,
            max_length=max_target_len,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_data = dataset.map(
    preprocess,
    batched=True,
    remove_columns=dataset["train"].column_names
)

# === 4. Evaluation metrics ===
rouge = load("rouge")

def compute_metrics(eval_pred):
    """Compute ROUGE scores and show random samples"""
    predictions, labels = eval_pred
    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Clean up decoded text
    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

    # Compute ROUGE metrics
    rouge_output = rouge.compute(
        predictions=decoded_preds,
        references=decoded_labels,
        use_stemmer=True,
        use_aggregator=True
    )

    # Show a few prediction samples
    if accelerator.is_local_main_process:
        print("\n=== Sample Predictions ===")
        for i in range(min(3, len(decoded_preds))):
            print(f"\nReference: {decoded_labels[i]}")
            print(f"Prediction: {decoded_preds[i]}")

    return {k: round(v, 4) for k, v in rouge_output.items()}

def postprocess_text(preds, labels):
    """Clean spacing and punctuation for Turkish summaries"""
    preds = [pred.strip() for pred in preds]
    labels = [label.strip() for label in labels]

    preds = [pred.replace(" .", ".").replace("..", ".") for pred in preds]
    labels = [label.replace(" .", ".").replace("..", ".") for label in labels]

    return preds, labels

# === 5. Training configuration ===
training_args = Seq2SeqTrainingArguments(
    output_dir="./turkish-t5-summarizer",
    eval_strategy="steps",
    eval_steps=500,
    save_steps=500,
    logging_steps=100,
    learning_rate=3e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,
    weight_decay=0.01,
    num_train_epochs=30,
    warmup_steps=500,
    predict_with_generate=True,
    fp16=torch.cuda.is_available(),
    load_best_model_at_end=True,
    metric_for_best_model="rougeLsum",
    greater_is_better=True,
    save_total_limit=3,
    report_to="none",
    seed=42
)

# === 6. Setup Trainer ===
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_data["train"],
    eval_dataset=tokenized_data["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)

# === 7. Train the model ===
print("Training started...")
train_result = trainer.train()

# === 8. Save the model and tokenizer ===
trainer.save_model("turkish_meta_summarizer")
tokenizer.save_pretrained("turkish_meta_summarizer")

# Save training logs and metrics
metrics = train_result.metrics
trainer.log_metrics("train", metrics)
trainer.save_metrics("train", metrics)
trainer.save_state()

print("Training completed! Model saved to: turkish_meta_summarizer")