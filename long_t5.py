# -*- coding: utf-8 -*-
"""LONG-T5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IqX6i0bbnzBwaXGSBC8MjpJ-TPFoPVJC
"""

import pandas as pd

# Specify the path to your Excel file
file_path = r"/content/2 documents_summary.xlsx"

# Read the Excel file
data_xlsx = pd.read_excel(file_path)

# Display the first few rows to verify the data
print(data_xlsx.head())
print(len(data_xlsx))
from datasets import Dataset

# Convert the dataset to a list of dictionaries
formatted_data = [{"document": doc, "summary": summary} for doc, summary in zip(data_xlsx['documents'], data_xlsx['summaries'])]

# Split the dataset into training and testing (80% training, 20% testing)
split_index = int(0.8 * len(formatted_data))
train_data = formatted_data[:split_index]
test_data = formatted_data[split_index:]

# Convert to Hugging Face Dataset format
train_dataset = Dataset.from_list(train_data)
test_dataset = Dataset.from_list(test_data)

# Print a sample to verify
print("\nSample from training dataset (before tokenization):", train_dataset[0])
print("\nSample from test dataset (before tokenization):", test_dataset[0])
import os
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments, EarlyStoppingCallback

tokenizer = AutoTokenizer.from_pretrained("google/long-t5-tglobal-base")
model = AutoModelForSeq2SeqLM.from_pretrained("google/long-t5-tglobal-base")

# Add a special padding token if it's not available
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    model.resize_token_embeddings(len(tokenizer))  # Resize the token embeddings to match the tokenizer

# Set the pad_token_id in the model configuration
model.config.pad_token_id = tokenizer.pad_token_id

# Define the preprocessing function
def preprocess_function(examples):
    # Prepare the input by prepending "summarize:" to the documents
    inputs = ["summarize: " + doc for doc in examples["document"]]

    # Tokenize the inputs
    model_inputs = tokenizer(
        inputs,
        max_length=2048,
        truncation=True,
        padding="max_length"
    )

    # Tokenize the summaries as labels
    labels = tokenizer(
        examples["summary"],
        max_length=700,
        truncation=True,
        padding="max_length"
    )

    # Store labels in the model_inputs dictionary
    model_inputs["labels"] = labels["input_ids"]

    return model_inputs

# Tokenize the training and test datasets
train_dataset = train_dataset.map(preprocess_function, batched=True)
test_dataset = test_dataset.map(preprocess_function, batched=True)

# Print a tokenized sample to verify
print(test_dataset[0])

# Disable Weights and Biases logging
os.environ["WANDB_DISABLED"] = "true"

# Check for GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Function to make model parameters contiguous (for optimization)
def make_contiguous(model):
    for param in model.parameters():
        if not param.is_contiguous():
            param.data = param.data.contiguous()

# Define the training arguments
training_args = TrainingArguments(
    output_dir=r"C:\Users\mahaa\OneDrive\Desktop\long-t5_TRAINING",
    evaluation_strategy="steps",             # Evaluate every few steps
    per_device_train_batch_size=4,           # Batch size per GPU/TPU core
    per_device_eval_batch_size=4,            # Batch size for evaluation
    num_train_epochs=100,                     # Number of training epochs
    save_steps=500,                          # Save checkpoint every 100 steps
    eval_steps=500,                          # Evaluate every 100 steps
    logging_dir='./logs',                    # Directory to save logs
    logging_steps=500,                       # Log every 100 steps
    logging_strategy="steps",                # Log every few steps
    save_total_limit=3,                      # Limit number of checkpoints to 3
    load_best_model_at_end=True,             # Load the best model based on eval metrics
    metric_for_best_model="eval_loss",       # Metric for determining the best model
    report_to="none"                         # Disable reporting to external services (like W&B)
)

# Custom Trainer class to make tensors contiguous before saving the model
class ContiguousTrainer(Trainer):
    def save_model(self, output_dir: str = None, _internal_call: bool = False):
        make_contiguous(self.model)  # Make all model parameters contiguous
        super().save_model(output_dir, _internal_call=_internal_call)

# Initialize the Trainer and proceed with training
trainer = ContiguousTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
)

# Start training
trainer.train()

# Print the training logs
for log in trainer.state.log_history:
    print(log)

# Save the fine-tuned model and tokenizer
model.save_pretrained("./long-t5_finetuned_summarization")
tokenizer.save_pretrained("./long-t5_finetuned_summarization")