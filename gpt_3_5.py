# -*- coding: utf-8 -*-
"""GPT-3.5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q3lNF8MyRoMEn_W6csnYqMAMsrRiPlhl
"""

# === 1. Imports ===
import os
import json
import torch
import pandas as pd
from datasets import Dataset
from sklearn.model_selection import train_test_split
from openai import OpenAI
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# === 2. Load and Explore the Dataset ===
file_path = "/content/2 documents_summary.xlsx"
data_xlsx = pd.read_excel(file_path)

print(data_xlsx.head(5))
print(f"Total records: {len(data_xlsx)}")

# Save a test split for later evaluation
_, test_data = train_test_split(data_xlsx, test_size=0.2, random_state=42)
test_data.to_excel("test_data.xlsx", index=False)
print(f"# Test examples: {len(test_data)}")

# === 3. Convert to GPT-3.5 Fine-Tuning Format ===
def convert_to_gpt35_format(dataset):
    fine_tuning_data = []
    for _, row in dataset.iterrows():
        json_response = '{"Summary": "' + row['summaries'] + '"}'
        fine_tuning_data.append({
            "messages": [
                {"role": "user", "content": row['documents']},
                {"role": "assistant", "content": json_response}
            ]
        })
    return fine_tuning_data

convert_dataset = convert_to_gpt35_format(data_xlsx)
print(convert_dataset[0]['messages'])
print(json.loads(convert_dataset[0]['messages'][-1]['content']))

# === 4. Split into Training and Validation Sets ===
train_data, val_data = train_test_split(convert_dataset, test_size=0.2, random_state=42)
print(f"# Training examples: {len(train_data)}")
print(f"# Validation examples: {len(val_data)}")

# === 5. Save as JSONL ===
def write_to_jsonl(data, file_path):
    with open(file_path, 'w', encoding='utf-8') as file:
        for entry in data:
            json.dump(entry, file, ensure_ascii=False)
            file.write('\n')

training_file_name = "train.jsonl"
validation_file_name = "val.jsonl"

write_to_jsonl(train_data, training_file_name)
write_to_jsonl(val_data, validation_file_name)

# === 6. Upload to OpenAI and Start Fine-Tuning Job ===
client = OpenAI(api_key="KEY")

training_file = client.files.create(file=open(training_file_name, "rb"), purpose="fine-tune")
validation_file = client.files.create(file=open(validation_file_name, "rb"), purpose="fine-tune")

suffix_name = "yt"
response = client.fine_tuning.jobs.create(
    training_file=training_file.id,
    validation_file=validation_file.id,
    model="gpt-3.5-turbo",
    suffix=suffix_name,
)

print("training_file_id:", training_file.id)
print("validation_file_id:", validation_file.id)

# === 7. Prediction on Test Data ===
def format_test(row):
    return [{"role": "user", "content": row['documents']}]

def predict(test_messages, fine_tuned_model_id):
    response = client.chat.completions.create(
        model=fine_tuned_model_id,
        messages=test_messages,
        temperature=0,
        max_tokens=700
    )
    return response.choices[0].message.content

def store_predictions(test_df, fine_tuned_model_id):
    test_df['Prediction'] = None
    for index, row in test_df.iterrows():
        test_message = format_test(row)
        prediction_result = predict(test_message, fine_tuned_model_id)
        test_df.at[index, 'Prediction'] = prediction_result
    test_df.to_excel("predictions.xlsx", index=False)

# Load test dataset and run predictions
test_df = pd.read_excel('test_data.xlsx')
store_predictions(test_df, fine_tuned_model_id="ID")