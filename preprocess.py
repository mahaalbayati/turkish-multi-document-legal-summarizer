# -*- coding: utf-8 -*-
"""preprocess.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZIG4HhlHsFuO8S_oRVWhLew9xA-p_Q5G
"""

#you could modify this code to work for all documents instead of one
import re
from zemberek import TurkishMorphology, TurkishSentenceExtractor, TurkishTokenizer

# Function to clean and preprocess the text
def clean_legal_document(text):
    keywords = ["taraflar", "Taraflar"]
    processed_lines = []
    start_copying = False

    for line in text.splitlines():
        if not start_copying:
            for keyword in keywords:
                if keyword in line:
                    start_copying = True
                    break
        if start_copying:
            processed_lines.append(line)

    # Join the processed lines into a single string
    cleaned_text = "\n".join(processed_lines)

    # Return the cleaned text as a single string
    return cleaned_text

# Tokenization and stop word removal with cleaning incorporated
def tokenize(text, stop_word):
    print("Original Document:\n", text)

    # Clean the document
    cleaned_text = clean_legal_document(text)

    word_extractor = TurkishTokenizer.DEFAULT
    sentence_extractor = TurkishSentenceExtractor()

    # Extract sentences
    raw_sentences = sentence_extractor.from_paragraph(cleaned_text)
    sentences = [sentence for sentence in raw_sentences if not re.match(r'\b\d+\.', sentence)]
    print("\nTokenized Sentences after removing the header: \n", sentences)

    # Tokenize and remove stop words
    tokens = word_extractor.tokenize(cleaned_text)
    token_contents = [token.content for token in tokens if token.content not in stop_word and not re.match(r'\b\d+\.', token.content)]

    # Join tokens back to form the cleaned document after stop words removal
    cleaned_document_after_stopwords = ' '.join(token_contents)
    text = cleaned_document_after_stopwords
    # Remove all occurrences of numbers followed by a period (e.g., "1.")
    text = re.sub(r'\b\d+\.\b', '', text)
    # Remove all other numbers
    text = re.sub(r'\b\d+\b', '', text)

    text = re.sub(r'\s{2,}', ' ', text)
    text = re.sub(r'\,\ \,', ' ', text)
    text = re.sub(r'\,\,', ' ', text)
    text = re.sub(r'\:', ' ', text)
    text = re.sub(r'\'', ' ', text)
    text = re.sub(r'\“', ' ', text)
    text = re.sub(r'\”', ' ', text)
    text = re.sub(r'\;', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'^\s+', '', text)
    text = re.sub(r'\bK\s+A\s+R\s+A\s+R\b', 'KARAR', text)
    text = re.sub(r'(\d+),(\d{2})\s*TL', r'\1.\2 TL', text)
    text = re.sub(r'\b\d+/\d+\.\ ', '', text)
    text = re.sub(r'-', '', text)
    text = re.sub(r'/', '', text)
    text = re.sub(r':', '', text)
    text = re.sub(r'\b\d+[a-zA-Z]\b', '', text)
    text = re.sub(r'\b(?:\d+[a-zA-Z]+)+\d*\b', '', text)
    text = text.lower()
    text = re.sub(r'\b(\w+)\b(?:\s+\1\b)+', r'\1', text)
    text = re.sub(r'\b(\w+)\b(?:\s+\w+){0,4}\s+\1\b', r'\1', text)

    text_tokens = word_extractor.tokenize(text)
    text_token_contents = [tok.content for tok in text_tokens if tok.content not in stop_word and not re.match(r'\b\d+\.', tok.content)]
    text_final = ' '.join(text_token_contents)
    text_final = text_final.replace(' ,', ',').replace(' .', '.').replace('., ', ' ').replace(',.', ',')

    print("\nTokenized Words after filtering stop word:\n", text_token_contents)
    print("\nDocument after cleaning and removing stop words:\n", text_final)

    return text_token_contents, sentences, text_final

# Sample input document
document = """
              document
          """

# Load the stop words from the specified file
stop_word = []
path = r"stopwords-tr.txt"
with open(path, 'r', encoding='utf-8') as file:
    stop_word = file.read().splitlines()

# Tokenize, clean, and remove stopwords
tokens, sentences, cleaned_document_after_stopwords = tokenize(document, stop_word)

# Morphological analysis and stemming
morphology = TurkishMorphology.create_with_defaults()
root_words = []

stemmed_tokens = []

for idx, word in enumerate(tokens):
    results = morphology.analyze(word)
    last_stem = None

    for result in results:
        last_stem = result.get_stem()

    if last_stem:
        stemmed_tokens.append(last_stem)
        root_words.append(last_stem)
    else:
        stemmed_tokens.append(word)
        root_words.append(word)

# Join the stemmed tokens back into a document
stemmed_document = ' '.join(stemmed_tokens)

# No need for replacements here because it's done earlier

print("\nDocument after stemming:\n", stemmed_document)