# -*- coding: utf-8 -*-
"""Evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nC3Vq0sQIFO_dKLjG9LZWtfXrRmb7Mej
"""

import evaluate
import torch
import pandas as pd
from bert_score import score as bert_score
from rouge_score import rouge_scores

# Load ROUGE metric
rouge_metric = evaluate.load("rouge")

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load tokenizer and model for the used model
tokenizer = Tokenizer.from_pretrained("./finetuned-legal")
model = Generation.from_pretrained("./finetuned-legal")
model.to(device)

# Function to generate a summary
def generate_summary(input_text, model, tokenizer, max_length=max):
    inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True, max_length=max_length).to(device)
    summary_ids = model.generate(inputs['input_ids'], max_length=max_length, num_beams=4, early_stopping=True)
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)

# Evaluation function
def evaluate_model(dataset, model, tokenizer, rouge_metric, max_length=max, save_path="generated_summaries.csv"):
    references = []
    predictions = []
    documents = []

    for example in dataset:
        document = example['document']
        reference_summary = example['summary']

        generated_summary = generate_summary(document, model, tokenizer, max_length=max_length)

        documents.append(document)
        references.append(reference_summary)
        predictions.append(generated_summary)

    # Compute ROUGE
    rouge_output = rouge_metric.compute(predictions=predictions, references=references)

    # Compute BERTScore (F1)
    print(" Calculating BERTScore using XLM-RoBERTa (not saved to file)...")
    _, _, F1 = bert_score(
        predictions,
        references,
        lang='en',
        model_type='xlm-roberta-large',
        device=device
    )
    avg_f1 = sum([f.item() for f in F1]) / len(F1)
    print(f"\nBERTScore F1 (average): {avg_f1:.4f}")

    # Save results to CSV (without saving BERTScore column)
    df = pd.DataFrame({
        "Document": documents,
        "Reference_Summary": references,
        "Generated_Summary": predictions
    })
    df.to_csv(save_path, index=False, encoding='utf-8-sig')

    return rouge_output

# Run evaluation
rouge_scores = evaluate_model(test_dataset, model, tokenizer, rouge_metric)

# Print ROUGE scores
print("\n Final ROUGE Scores:")
for key, value in rouge_scores.items():
    if isinstance(value, dict):
        print(f"{key}:")
        for sub_key, sub_value in value.items():
            print(f"  {sub_key}: {sub_value}")
    else:
        print(f"{key}: {value}")