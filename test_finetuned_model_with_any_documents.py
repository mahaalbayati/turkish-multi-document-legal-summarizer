# -*- coding: utf-8 -*-
"""test_finetuned_model_with_any_documents.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BeaDfidv7Z3G96lHLYCRQN3uqbBxd4X5
"""

# Move model to the correct device (GPU or CPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

tokenizer = Tokenizer.from_pretrained("./finetuned-legal")
model = Generation.from_pretrained("./finetuned-legal")


# Function to generate summary for long inputs using LED
def generate_multi_document_summary_with_led(documents, model, tokenizer, max_length=max, num_beams=num):
    # Remove commas from each document
    processed_documents = [doc.replace(",", "") for doc in documents]

    # Concatenate the processed documents into a single string
    concatenated_docs = " ".join(processed_documents)

    # Tokenize the concatenated documents with the larger max_length supported by LED
    inputs = tokenizer(concatenated_docs, return_tensors="pt", padding=True, truncation=True, max_length=max).to(device)

    # Generate summary
    summary_ids = model.generate(
        inputs['input_ids'],
        min_length=min,
        max_length=max_length,
        num_beams=num_beams,
        early_stopping=True  # Early stopping can now be enabled safely
    )

    # Decode the generated summary
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)

    return summary

# Example of multiple documents
documents = [
              """document1""", """document2"""
            ]

# Generate the summary using LED
generated_summary = generate_multi_document_summary_with_finetuned-legal(documents, model, tokenizer)
print("Generated Summary:")
print(generated_summary)