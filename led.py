# -*- coding: utf-8 -*-
"""LED.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17NGIeG0WPzbjsZ4e5kvR1CADNc_8K_ka
"""

import pandas as pd
from transformers import Trainer, EarlyStoppingCallback
import os
import torch
from transformers import LEDForConditionalGeneration, LEDTokenizer, Trainer, TrainingArguments

os.environ["WANDB_DISABLED"] = "true"

# Specify the path to your Excel file
file_path = r"/content/2 documents_summary.xlsx"

# Read the Excel file
data_xlsx = pd.read_excel(file_path)

# Display the first few rows to verify the data
print(data_xlsx.head())
print(len(data_xlsx))
from datasets import Dataset

# Convert the dataset to a list of dictionaries where each dictionary contains 4 documents and their corresponding summary
formatted_data = [{"document": doc, "summary": summary} for doc, summary in zip(data_xlsx['documents'], data_xlsx['summaries'])]

# Split the dataset into training and testing (80% training, 20% testing)
split_index = int(0.8 * len(formatted_data))
train_data = formatted_data[:split_index]
test_data = formatted_data[split_index:]

# Convert to Hugging Face Dataset format
train_dataset = Dataset.from_list(train_data)
test_dataset = Dataset.from_list(test_data)

# Print a sample to verify
print("\nSample from training dataset (before tokenization):", train_dataset[0])
print("\nSample from test dataset (before tokenization):", test_dataset[0])


# Load the LED tokenizer and model
model_name = "allenai/led-base-16384"
tokenizer = LEDTokenizer.from_pretrained(model_name)
model = LEDForConditionalGeneration.from_pretrained(model_name)

# Define a function to tokenize the dataset
def preprocess_function(examples):
    # Prepare the input by prepending "summarize:" to the documents (optional for LED, can remove if not needed)
    inputs = ["summarize: " + doc for doc in examples["document"]]

    # Tokenize the inputs (documents)
    model_inputs = tokenizer(inputs, max_length=2048, truncation=True, padding="max_length")

    # Tokenize the summaries (as labels)
    labels = tokenizer(examples["summary"], max_length=700, truncation=True, padding="max_length")

    # Replace the padding token in labels with -100 to ignore these during loss calculation
    labels["input_ids"] = [
        [(label if label != tokenizer.pad_token_id else -100) for label in summary] for summary in labels["input_ids"]
    ]

    # Store the labels in the model_inputs dictionary
    model_inputs["labels"] = labels["input_ids"]

    return model_inputs

# Tokenize the training and test datasets
train_dataset = train_dataset.map(preprocess_function, batched=True)
test_dataset = test_dataset.map(preprocess_function, batched=True)

# Print a tokenized sample to verify
print(train_dataset[0])


# Check for GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

model = "allenai/led-base-16384"
tokenizer = LEDTokenizer.from_pretrained(model)
model = LEDForConditionalGeneration.from_pretrained(model)

training_args = TrainingArguments(
    output_dir="./allenai_TRAINING",
    eval_strategy="steps",             # Evaluate every few steps
    per_device_train_batch_size=4,           # Batch size per GPU/TPU core
    per_device_eval_batch_size=4,            # Batch size for evaluation
    num_train_epochs=100,                     # Number of training epochs
    save_steps=500,                          # Save checkpoint every 100 steps
    eval_steps=500,                          # Evaluate every 100 steps
    logging_dir='./logs',                    # Directory to save logs
    logging_steps=500,                       # Log every 100 steps
    logging_strategy="steps",                # Log every few steps
    save_total_limit=3,                      # Limit number of checkpoints to 3
    load_best_model_at_end=True,             # Load the best model based on eval metrics
    metric_for_best_model="eval_loss",       # Metric for determining the best model
    report_to="none"                         # Disable reporting to external services (like W&B)
    )


# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,  # You can provide a validation dataset for evaluation
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
               )

# Train the model
trainer.train()

trainer.evaluate()

# Save the fine-tuned model
trainer.save_model("./led-base-finetuned-legal")
model.save_pretrained("./led-base-finetuned-legal")
tokenizer.save_pretrained("./led-base-finetuned-legal")